# ğŸ¤ Speech Emotion Analysis for Workplace Wellness

[![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)](https://www.python.org/downloads/)
[![Flask](https://img.shields.io/badge/flask-v2.0+-green.svg)](https://flask.palletsprojects.com/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)

> An AI-powered speech emotion recognition system designed to enhance workplace wellness through real-time emotional state monitoring and analysis.

![Demo](assets/demo.gif) <!-- Add your demo GIF here -->

## ğŸŒŸ Features

- ğŸ¯ **Real-time Emotion Detection**: Analyze emotions from voice recordings with 8 emotion categories
- ğŸ”’ **Privacy-First Design**: No permanent audio storage, secure processing
- ğŸŒ **Web-Based Interface**: User-friendly HTML5 interface with audio recording
- âš¡ **Fast Processing**: Optimized feature extraction with <30s processing time
- ğŸ›¡ï¸ **Robust Error Handling**: Comprehensive timeout and error management
- ğŸ“Š **Multiple Audio Formats**: Supports WAV, MP3, FLAC, OGG, M4A
- ğŸ”„ **RESTful API**: Clean API endpoints for integration

## ğŸ¯ Supported Emotions

| Emotion | Description | Use Case |
|---------|-------------|----------|
| ğŸ˜ Neutral | Calm, balanced state | Baseline measurement |
| ğŸ˜Œ Calm | Relaxed, peaceful | Stress level monitoring |
| ğŸ˜Š Happy | Positive, cheerful | Satisfaction tracking |
| ğŸ˜¢ Sad | Melancholy, down | Mental health alerts |
| ğŸ˜  Angry | Frustrated, irritated | Conflict detection |
| ğŸ˜¨ Fearful | Anxious, worried | Stress identification |
| ğŸ¤¢ Disgust | Displeased, repulsed | Workplace satisfaction |
| ğŸ˜² Surprised | Shocked, amazed | Engagement levels |

## ğŸš€ Quick Start

### Prerequisites

```bash
Python 3.8+
pip package manager
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/speech-emotion-analysis.git
cd speech-emotion-analysis
```

2. **Create virtual environment**
```bash
python -m venv venv

# On Windows
venv\Scripts\activate

# On macOS/Linux
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Download the pre-trained model**
```bash
# Option 1: Download from releases
wget https://github.com/yourusername/speech-emotion-analysis/releases/download/v1.0/voice_analyzer_model.pkl

# Option 2: Train your own model (see Training section)
python train_model.py
```

5. **Run the application**
```bash
python app.py
```

6. **Open your browser**
Navigate to `http://localhost:5000`

## ğŸ“ Project Structure

```
speech-emotion-analysis/
â”‚
â”œâ”€â”€ ğŸ“ static/                 # Frontend assets
â”‚   â”œâ”€â”€ style.css             # CSS styling
â”‚   â”œâ”€â”€ script.js             # JavaScript functionality
â”‚   â””â”€â”€ images/               # UI images
â”‚
â”œâ”€â”€ ğŸ“ templates/              # HTML templates
â”‚   â””â”€â”€ index.html            # Main interface
â”‚
â”œâ”€â”€ ğŸ“ uploads/                # Temporary audio files
â”‚   â””â”€â”€ .gitkeep              # Keep directory in git
â”‚
â”œâ”€â”€ ğŸ“ models/                 # ML models and training
â”‚   â”œâ”€â”€ train_model.py        # Model training script
â”‚   â”œâ”€â”€ feature_extraction.py # Audio feature utilities
â”‚   â””â”€â”€ voice_analyzer_model.pkl # Pre-trained model
â”‚
â”œâ”€â”€ ğŸ“ tests/                  # Unit tests
â”‚   â”œâ”€â”€ test_app.py           # Flask app tests
â”‚   â”œâ”€â”€ test_features.py      # Feature extraction tests
â”‚   â””â”€â”€ sample_audio/         # Test audio files
â”‚
â”œâ”€â”€ ğŸ“ docs/                   # Documentation
â”‚   â”œâ”€â”€ API.md                # API documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md         # Deployment guide
â”‚   â””â”€â”€ TECHNICAL_REPORT.md   # Detailed technical report
â”‚
â”œâ”€â”€ ğŸ“ assets/                 # README assets
â”‚   â”œâ”€â”€ demo.gif              # Demo animation
â”‚   â”œâ”€â”€ architecture.png      # System architecture
â”‚   â””â”€â”€ screenshots/          # UI screenshots
â”‚
â”œâ”€â”€ app.py                    # Main Flask application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ config.py                 # Configuration settings
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ .gitignore               # Git ignore rules
â”œâ”€â”€ LICENSE                  # MIT License
â”œâ”€â”€ README.md               # This file
â””â”€â”€ CONTRIBUTING.md         # Contribution guidelines
```

## ğŸ› ï¸ Usage

### Web Interface

1. **Record Audio**: Click the microphone button to start recording
2. **Upload File**: Or upload an existing audio file (max 16MB)
3. **Analyze**: Click "Analyze Emotion" to process
4. **View Results**: See the detected emotion with confidence score

### API Usage

#### Analyze Audio File
```python
import requests

# Upload and analyze audio file
files = {'audio': open('sample.wav', 'rb')}
response = requests.post('http://localhost:5000/analyze', files=files)
result = response.json()

print(f"Detected Emotion: {result['emotion']}")
```

#### Health Check
```python
import requests

response = requests.get('http://localhost:5000/test')
print(response.json())
```

### Command Line Usage

```bash
# Run with custom configuration
python app.py --host 0.0.0.0 --port 8080

# Run in production mode
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file based on `.env.example`:

```bash
# Server Configuration
FLASK_ENV=development
FLASK_DEBUG=False
PORT=5000
HOST=0.0.0.0

# Upload Configuration
MAX_CONTENT_LENGTH=16777216  # 16MB
UPLOAD_FOLDER=uploads

# Model Configuration
MODEL_PATH=voice_analyzer_model.pkl
FEATURE_EXTRACTION_TIMEOUT=30

# Security
SECRET_KEY=your-secret-key-here
```

### Custom Configuration

```python
# config.py
class Config:
    MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB
    UPLOAD_FOLDER = 'uploads'
    MODEL_PATH = 'voice_analyzer_model.pkl'
    
class DevelopmentConfig(Config):
    DEBUG = True
    TESTING = False
    
class ProductionConfig(Config):
    DEBUG = False
    TESTING = False
```

## ğŸ§ª Testing

### Run Tests

```bash
# Run all tests
python -m pytest tests/ -v

# Run specific test file
python -m pytest tests/test_app.py -v

# Run with coverage
python -m pytest tests/ --cov=. --cov-report=html
```

### Test API Endpoints

```bash
# Health check
curl http://localhost:5000/test

# Upload test audio
curl -X POST -F "audio=@tests/sample_audio/happy.wav" http://localhost:5000/analyze
```

## ğŸ“Š Model Training

### Dataset Preparation

```bash
# Organize your dataset
dataset/
â”œâ”€â”€ angry/
â”œâ”€â”€ happy/
â”œâ”€â”€ sad/
â”œâ”€â”€ neutral/
â””â”€â”€ ...
```

### Training Process

```python
# Train new model
python models/train_model.py --dataset_path dataset/ --output_path voice_analyzer_model.pkl

# Evaluate model
python models/evaluate_model.py --model_path voice_analyzer_model.pkl --test_data test_dataset/
```

### Feature Extraction Details

The system extracts 32-dimensional feature vectors:
- **MFCC Features**: 13 coefficients (spectral characteristics)
- **Chroma Features**: 12 coefficients (harmonic content)  
- **Spectral Contrast**: 7 coefficients (spectral peak valleys)

## ğŸš€ Deployment

### Docker Deployment

```bash
# Build image
docker build -t speech-emotion-analysis .

# Run container
docker run -p 5000:5000 speech-emotion-analysis
```

### Production Deployment

```bash
# Install production server
pip install gunicorn

# Run with Gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 --timeout 60 app:app

# Or use the provided script
./deploy.sh
```

### Cloud Deployment

#### Heroku
```bash
# Login and create app
heroku login
heroku create your-app-name

# Deploy
git push heroku main
```

#### AWS/GCP/Azure
See [DEPLOYMENT.md](docs/DEPLOYMENT.md) for detailed cloud deployment guides.

## ğŸ” API Documentation

### Endpoints

| Method | Endpoint | Description | Parameters |
|--------|----------|-------------|------------|
| GET | `/` | Main interface | - |
| POST | `/analyze` | Analyze audio | `audio`: Audio file |
| GET | `/test` | Health check | - |

### Response Format

```json
{
  "emotion": "Happy",
  "confidence": 0.85,
  "processing_time": "2.3s",
  "timestamp": "2025-07-19T10:30:00Z"
}
```

### Error Responses

```json
{
  "error": "Audio processing timed out",
  "code": 408,
  "message": "Please try a shorter audio file"
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Fork the repository
# Clone your fork
git clone https://github.com/yourusername/speech-emotion-analysis.git

# Create feature branch
git checkout -b feature/amazing-feature

# Make changes and test
python -m pytest tests/

# Commit and push
git commit -m "Add amazing feature"
git push origin feature/amazing-feature

# Create Pull Request
```

### Contribution Areas

- ğŸ› Bug fixes and improvements
- âœ¨ New emotion categories
- ğŸŒ Multi-language support
- ğŸ“± Mobile app development
- ğŸ§  Advanced ML models
- ğŸ“š Documentation improvements

## ğŸ“ˆ Performance

### Benchmarks

| Metric | Value |
|--------|-------|
| Average Processing Time | 2.5s |
| Accuracy | 87.3% |
| Supported File Size | Up to 16MB |
| Concurrent Users | 50+ |
| Memory Usage | <512MB |

### Optimization Tips

- Use shorter audio clips (10-30 seconds) for faster processing
- WAV format provides best accuracy
- Ensure good audio quality for better results
- Consider batch processing for multiple files

## ğŸ”’ Privacy & Security

### Privacy Features
- âœ… No permanent audio storage
- âœ… Temporary file cleanup
- âœ… No user tracking
- âœ… Local processing option
- âœ… GDPR compliant design

### Security Measures
- ğŸ›¡ï¸ File type validation
- ğŸ›¡ï¸ Size limitations
- ğŸ›¡ï¸ Secure filename handling
- ğŸ›¡ï¸ Input sanitization
- ğŸ›¡ï¸ Rate limiting support

## ğŸ› Troubleshooting

### Common Issues

#### Model Loading Error
```bash
âŒ Error loading model: No such file or directory: 'voice_analyzer_model.pkl'
```
**Solution**: Download the pre-trained model or train a new one.

#### Audio Processing Timeout
```bash
âŒ Audio processing timed out
```
**Solution**: Use shorter audio files or increase timeout in config.

#### Permission Errors
```bash
âŒ Permission denied: uploads/
```
**Solution**: Ensure proper file permissions for upload directory.

### Debug Mode

```bash
# Run in debug mode
FLASK_DEBUG=True python app.py

# Check logs
tail -f logs/app.log
```

## ğŸ“š Resources

### Documentation
- [Technical Report](docs/TECHNICAL_REPORT.md)
- [API Documentation](docs/API.md)
- [Deployment Guide](docs/DEPLOYMENT.md)

### Research Papers
- [Emotion Recognition in Speech](https://example.com/paper1)
- [Audio Feature Extraction](https://example.com/paper2)
- [Workplace Wellness Applications](https://example.com/paper3)

### Tutorials
- [Building Speech Recognition Systems](https://example.com/tutorial1)
- [Flask Web Development](https://example.com/tutorial2)
- [Machine Learning for Audio](https://example.com/tutorial3)

## ğŸ“Š Roadmap

### Version 2.0 (Q3 2025)
- [ ] Real-time streaming analysis
- [ ] Mobile application
- [ ] Advanced deep learning models
- [ ] Multi-language support

### Version 3.0 (Q1 2026)
- [ ] Wearable device integration
- [ ] Team emotion analytics
- [ ] Predictive wellness modeling
- [ ] Enterprise dashboard

## â“ FAQ

### Q: How accurate is the emotion detection?
A: The system achieves ~87% accuracy on test datasets, though real-world performance may vary based on audio quality and speaker characteristics.

### Q: What audio quality is required?
A: Best results with clear audio, minimal background noise, sample rate â‰¥16kHz. Phone recordings typically work well.

### Q: Can I train my own model?
A: Yes! Use the provided training scripts with your own dataset. See the Model Training section.

### Q: Is this HIPAA compliant?
A: The system is designed with privacy in mind, but HIPAA compliance requires additional security measures. Consult your compliance team.

### Q: Can I use this commercially?
A: Yes, under the MIT license. See LICENSE file for details.

## ğŸ† Acknowledgments

- [Librosa](https://librosa.org/) for audio processing
- [scikit-learn](https://scikit-learn.org/) for machine learning
- [Flask](https://flask.palletsprojects.com/) for web framework
- RAVDESS dataset for training data
- Open source community for inspiration

## ğŸ“ Support

- ğŸ“§ Email: [muhammadsudaiskhalid.artificialintelligence@stmu.edu.pk](muhammadsudaiskhalid.artificialintelligence@stmu.edu.pk)
- ğŸ’¬ Discord: [Join our community](https://discord.com/invite/cfjfrec9)
- ğŸ› Issues: [GitHub Issues](https://github.com/yourusername/speech-emotion-analysis/issues)
- ğŸ“– Wiki: [Project Wiki](https://github.com/yourusername/speech-emotion-analysis/wiki)
- ğŸ’¬ LinkedIn : [LinkedIn](https://www.linkedin.com/in/sudais-khalid/)

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**If you found this project helpful, please consider giving it a â­!**

[â¬† Back to Top](#-speech-emotion-analysis-for-workplace-wellness)

</div>
